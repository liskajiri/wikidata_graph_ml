{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import torch_geometric\n",
    "import torch\n",
    "\n",
    "from constants import *\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikidata5m(InMemoryDataset):\n",
    "    train = \"wikidata5m_transductive_train.txt\"\n",
    "    validate = \"wikidata5m_transductive_valid.txt\"\n",
    "    test = \"wikidata5m_transductive_test.txt\"\n",
    "    corpus = \"wikidata5m_text.txt\"\n",
    "\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        # Root is a path to a folder which contains the datasets\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [self.train, self.validate, self.test, self.corpus]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # return self.processed_paths[0]\n",
    "        return [\"saved_dataset.pt\"]\n",
    "\n",
    "    # def download(self):\n",
    "    #     # Download to `self.raw_dir`.\n",
    "    #     download_url(url, self.raw_dir)\n",
    "    #     ...\n",
    "\n",
    "    def read_csv_files(self) -> list[pl.DataFrame]:\n",
    "        entity_columns =[\"entity1\", \"relation\", \"entity2\"] \n",
    "        def read_entity_file(file_name: str, entity_columns=entity_columns) -> pl.DataFrame:\n",
    "            return pl.read_csv(file_name, sep=\"\\t\", has_header=False, new_columns=entity_columns)\n",
    "\n",
    "        train_data = read_entity_file(DatasetPaths.train.value)\n",
    "        validate_data = read_entity_file(DatasetPaths.validate.value)\n",
    "        test_data = read_entity_file(DatasetPaths.test.value)\n",
    "        corpus_text = pl.read_csv(DatasetPaths.corpus.value, sep=\"\\t\", has_header=False, new_columns=[\"id\", \"description\"], n_rows=1000)\n",
    "\n",
    "        data = [train_data, validate_data, test_data, corpus_text]\n",
    "        data = [self.convert_df_to_int_indexes(dato) for dato in data]\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_df_to_int_indexes(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        int_columns = [\"id\", \"entity1\", \"entity2\", \"relation\"]\n",
    "        for col in df.columns:\n",
    "            if col in int_columns:\n",
    "                # Remove first element and convert col to int\n",
    "                df = df.with_columns(pl.col(col).str.lstrip(\"QP\").cast(pl.UInt32))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_edges_from_dataset(dataset: pl.DataFrame) -> np.array:\n",
    "        # PyG requires format [2, num_edges]\n",
    "        edges = dataset.select([\"entity1\", \"entity2\"]).to_numpy().T\n",
    "        assert(edges.shape[0] == 2)\n",
    "        return edges\n",
    "\n",
    "    def process(self):\n",
    "        train_data, validate_data, test_data, corpus_text = self.read_csv_files()\n",
    "\n",
    "        train_edges = self.get_edges_from_dataset(train_data)\n",
    "        test_edges = self.get_edges_from_dataset(test_data).T\n",
    "        nodes = corpus_text.select(\"id\").to_numpy()\n",
    "\n",
    "        data_list = [Data(x=nodes, edge_index=train_edges, y=test_edges)]\n",
    "\n",
    "        # if self.pre_filter is not None:\n",
    "        #     data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "        # if self.pre_transform is not None:\n",
    "        #     data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1000, 1], edge_index=[2, 20614279], y=[5133, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Wikidata5m(\"datasets\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c38629fd2b216446fae8efc32f1aa275963cb5ebed014889599f2e40172abad5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
